{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse Data Files:\n",
    "Imports tools to analyze OOI netCDF files and provide summary outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define folder path to save summary output\n",
    "sDir =  '/Users/leila/Documents/NSFEduSupport/review/output'\n",
    "# THERDD server contains the netCDF data files\n",
    "url_list = ['https://opendap.oceanobservatories.org/thredds/catalog/ooi/leila.ocean@gmail.com/20190319T195519-CP05MOAS-GL335-03-CTDGVM000-recovered_host-ctdgv_m_glider_instrument_recovered/catalog.html',\n",
    "            'https://opendap.oceanobservatories.org/thredds/catalog/ooi/leila.ocean@gmail.com/20190319T195533-CP05MOAS-GL335-03-CTDGVM000-telemetered-ctdgv_m_glider_instrument/catalog.html'] \n",
    "# review file was created upfront to do this analysis\n",
    "review_file = 'https://raw.githubusercontent.com/ooi-data-lab/data-review-prep/master/review_list/data_review_list.csv'\n",
    "# f =  #location to a file containing THREDDs urls with .nc files to analyze. \n",
    "#The column containing the THREDDs urls must be labeled 'outputUrl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leila/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: UserWarning: \n",
      "WARNING: Backwards incompatible files will be created with HDF5 1.10.x \n",
      "and netCDF < 4.4.1. Upgrading to netCDF4 >= 4.4.1 or downgrading to \n",
      "to HDF5 version 1.8.x is highly recommended \n",
      "(see https://github.com/Unidata/netcdf-c/issues/250).\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import json\n",
    "import datetime as dt\n",
    "import netCDF4 as nc\n",
    "import functions.common as cf\n",
    "import functions.plotting as pf\n",
    "from datetime import timedelta\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete the analysis by reference designator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewlist = pd.read_csv(review_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CP05MOAS-GL335-03-CTDGVM000-recovered_host-ctdgv_m_glider_instrument_recovered\n",
      "      deploymentNumber        startDateTime         stopDateTime in_am\n",
      "3694               1.0  2014-10-06T20:16:00  2014-12-15T00:00:00   yes\n",
      "3695               2.0  2015-10-13T01:12:14  2015-11-16T00:00:00   yes\n",
      "3696               3.0  2016-04-04T18:57:02  2016-04-18T00:00:00   yes\n",
      "3697               4.0  2016-05-27T20:33:00  2016-06-27T00:00:00   yes\n",
      "3698               5.0  2017-01-16T14:59:00  2017-03-06T22:45:00   yes\n",
      "Data request has fulfilled.\n",
      "CP05MOAS-GL335-03-CTDGVM000-telemetered-ctdgv_m_glider_instrument\n",
      "      deploymentNumber        startDateTime         stopDateTime in_am\n",
      "3694               1.0  2014-10-06T20:16:00  2014-12-15T00:00:00   yes\n",
      "3695               2.0  2015-10-13T01:12:14  2015-11-16T00:00:00   yes\n",
      "3696               3.0  2016-04-04T18:57:02  2016-04-18T00:00:00   yes\n",
      "3697               4.0  2016-05-27T20:33:00  2016-06-27T00:00:00   yes\n",
      "3698               5.0  2017-01-16T14:59:00  2017-03-06T22:45:00   yes\n",
      "Data request has fulfilled.\n"
     ]
    }
   ],
   "source": [
    "for uu in url_list:\n",
    "    # get instrument  = reference designator \n",
    "    elements = uu.split('/')[-2].split('-')\n",
    "    rd = '-'.join((elements[1], elements[2], elements[3], elements[4]))\n",
    "    \n",
    "    data = OrderedDict(deployments=OrderedDict())\n",
    "    \n",
    "    # create an output file\n",
    "    save_dir = os.path.join(sDir, rd.split('-')[0], rd)\n",
    "    cf.create_dir(save_dir)\n",
    "    \n",
    "    # check for the OOI 1.0 datasets for review    \n",
    "    rl_filtered = reviewlist.loc[(reviewlist['Reference Designator'] == rd) & (reviewlist['status'] == 'for review')]\n",
    "    \n",
    "    # print to the screen\n",
    "    catalog_rms = '-'.join((rd, elements[-2], elements[-1]))\n",
    "    print(catalog_rms)\n",
    "    print(pd.DataFrame({'deploymentNumber': rl_filtered['deploymentNumber'],\n",
    "                        'startDateTime': rl_filtered['startDateTime'],\n",
    "                       'stopDateTime': rl_filtered['stopDateTime'],\n",
    "                       'in_am': rl_filtered['in_am']}))\n",
    "    \n",
    "        \n",
    "    # get data files from THREDDS server\n",
    "    udatasets = cf.get_nc_urls([uu])\n",
    "    \n",
    "    # get deployments from file names\n",
    "    review_deployments = rl_filtered['deploymentNumber'].tolist()\n",
    "    review_deployments_int = ['deployment%04d' % int(x) for x in review_deployments]\n",
    "\n",
    "    # get data files of interest\n",
    "    datasets = []\n",
    "    for rev_dep in review_deployments_int:\n",
    "        rdatasets = [s for s in udatasets if rev_dep in s]\n",
    "        if len(rdatasets) > 0:            \n",
    "            for dss in rdatasets:  # filter out collocated data files\n",
    "                if catalog_rms == dss.split('/')[-1].split('_20')[0][15:]:\n",
    "                    datasets.append(dss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data file\n",
    "notes = []\n",
    "time_ascending = ''\n",
    "if len(datasets) != 0 & len(datasets) == 1:\n",
    "    ds = xr.open_dataset(datasets[0], mask_and_scale=False)\n",
    "    ds = ds.swap_dims({'obs': 'time'})\n",
    "    fname, subsite, refdes, method, data_stream, deployment = cf.nc_attributes(datasets[0])\n",
    "else: \n",
    "    ds = xr.open_mfdataset(datasets, mask_and_scale=False)\n",
    "    ds = ds.swap_dims({'obs': 'time'})\n",
    "    ds = ds.chunk({'time': 100})\n",
    "    fname, subsite, refdes, method, data_stream, deployment = cf.nc_attributes(datasets[0])\n",
    "    fname = fname.split('_20')[0]\n",
    "    notes.append('multiple deployment .nc files')\n",
    "    # when opening multiple datasets, don't check that the timestamps are in ascending order\n",
    "    time_ascending = 'not_tested'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Get info from the data review database\n",
    "dr_data = cf.refdes_datareview_json(refdes)\n",
    "stream_vars = cf.return_stream_vars(data_stream)\n",
    "sci_vars = cf.return_science_vars(data_stream)\n",
    "deploy_info = cf.get_deployment_information(dr_data, int(deployment[-4:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab deployment Variables\n",
    "deploy_start = str(deploy_info['start_date'])\n",
    "deploy_stop = str(deploy_info['stop_date'])\n",
    "deploy_lon = deploy_info['longitude']\n",
    "deploy_lat = deploy_info['latitude']\n",
    "deploy_depth = deploy_info['deployment_depth']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate days deployed\n",
    "if deploy_stop != 'None':\n",
    "    r_deploy_start = pd.to_datetime(deploy_start).replace(hour=0, minute=0, second=0)\n",
    "    if deploy_stop.split('T')[1] == '00:00:00':\n",
    "        r_deploy_stop = pd.to_datetime(deploy_stop)\n",
    "    else:\n",
    "        r_deploy_stop = (pd.to_datetime(deploy_stop) + timedelta(days=1)).replace(hour=0, minute=0, second=0)\n",
    "    n_days_deployed = (r_deploy_stop - r_deploy_start).days\n",
    "else:\n",
    "    n_days_deployed = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71\n"
     ]
    }
   ],
   "source": [
    "print(n_days_deployed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add reference designator to dictionary\n",
    "try:\n",
    "    data['refdes']\n",
    "except KeyError:\n",
    "    data['refdes'] = refdes\n",
    "\n",
    "deployments = data['deployments'].keys()\n",
    "data_start = pd.to_datetime(min(ds['time'].values)).strftime('%Y-%m-%dT%H:%M:%S')\n",
    "data_stop = pd.to_datetime(max(ds['time'].values)).strftime('%Y-%m-%dT%H:%M:%S')\n",
    "\n",
    "# Add deployment and info to dictionary and initialize delivery method sub-dictionary\n",
    "if deployment not in deployments:\n",
    "    data['deployments'][deployment] = OrderedDict(deploy_start=deploy_start,\n",
    "                                                  deploy_stop=deploy_stop,\n",
    "                                                  n_days_deployed=n_days_deployed,\n",
    "                                                  lon=deploy_lon,\n",
    "                                                  lat=deploy_lat,\n",
    "                                                  deploy_depth=deploy_depth,\n",
    "                                                  method=OrderedDict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add delivery methods to dictionary and initialize stream sub-dictionary\n",
    "methods = data['deployments'][deployment]['method'].keys()\n",
    "if method not in methods:\n",
    "    data['deployments'][deployment]['method'][method] = OrderedDict(\n",
    "        stream=OrderedDict())\n",
    "    \n",
    "# Add streams to dictionary and initialize file sub-dictionary\n",
    "streams = data['deployments'][deployment]['method'][method]['stream'].keys()\n",
    "if data_stream not in streams:\n",
    "    data['deployments'][deployment]['method'][method]['stream'][\n",
    "    data_stream] = OrderedDict(file=OrderedDict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of data gaps >1 day\n",
    "time_df = pd.DataFrame(ds['time'].values, columns=['time'])\n",
    "gap_list = cf.timestamp_gap_test(time_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the sampling rate to the nearest second\n",
    "time_df['diff'] = time_df['time'].diff().astype('timedelta64[s]')\n",
    "rates_df = time_df.groupby(['diff']).agg(['count'])\n",
    "n_diff_calc = len(time_df) - 1\n",
    "rates = dict(n_unique_rates=len(rates_df), common_sampling_rates=dict())\n",
    "for i, row in rates_df.iterrows():\n",
    "    percent = (float(row['time']['count']) / float(n_diff_calc))\n",
    "    if percent > 0.1:\n",
    "        rates['common_sampling_rates'].update({int(i): '{:.2%}'.format(percent)})\n",
    "sampling_rt_sec = None\n",
    "for k, v in rates['common_sampling_rates'].items():\n",
    "    if float(v.strip('%')) > 50.00:\n",
    "        sampling_rt_sec = k\n",
    "\n",
    "if not sampling_rt_sec:\n",
    "    sampling_rt_sec = 'no consistent sampling rate: {}'.format(rates['common_sampling_rates'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the timestamps in the file are unique\n",
    "time = ds['time']\n",
    "len_time = time.__len__()\n",
    "len_time_unique = np.unique(time).__len__()\n",
    "if len_time == len_time_unique:\n",
    "    time_test = 'pass'\n",
    "else:\n",
    "    time_test = 'fail'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the timestamps in the file are in ascending order\n",
    "if time_ascending != 'not_tested':\n",
    "    # convert time to number\n",
    "    time_in = [dt.datetime.utcfromtimestamp(np.datetime64(x).astype('O')/1e9) for x in\n",
    "               ds['time'].values]\n",
    "    time_data = nc.date2num(time_in, 'seconds since 1900-01-01')\n",
    "\n",
    "    # Create a list of True or False by iterating through the array of time and checking\n",
    "    # if every time stamp is increasing\n",
    "    result = [(time_data[k + 1] - time_data[k]) > 0 for k in range(len(time_data) - 1)]\n",
    "\n",
    "    # Print outcome of the iteration with the list of indices when time is not increasing\n",
    "    if result.count(True) == len(time) - 1:\n",
    "        time_ascending = 'pass'\n",
    "    else:\n",
    "        ind_fail = {k: time_in[k] for k, v in enumerate(result) if v is False}\n",
    "        time_ascending = 'fail: {}'.format(ind_fail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of days for which there is at least 1 timestamp\n",
    "n_days = len(np.unique(time.values.astype('datetime64[D]')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "190"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare variables in file to variables in Data Review Database\n",
    "ds_variables = list(ds.data_vars.keys()) + list(ds.coords.keys())\n",
    "#ds_variables = [k for k in ds]\n",
    "ds_variables = cf.eliminate_common_variables(ds_variables)\n",
    "ds_variables = [x for x in ds_variables if 'qc' not in x]\n",
    "[_, unmatch1] = cf.compare_lists(stream_vars, ds_variables)\n",
    "[_, unmatch2] = cf.compare_lists(ds_variables, stream_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check deployment pressure from asset management against pressure variable in file\n",
    "press = pf.pressure_var(ds, list(ds.coords.keys()))\n",
    "if press is None:\n",
    "    press = pf.pressure_var(ds, list(ds.data_vars.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "uFrame is not responding to request for global ranges. Try again later.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-0627b6aaecf9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m# reject data outside of global ranges\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0;34m[\u001b[0m\u001b[0mpg_min\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpg_max\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_global_ranges\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpress\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpg_min\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mpg_max\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/NSFEduSupport/github/8thEGOMeeting-notebooks/functions/common.py\u001b[0m in \u001b[0;36mget_global_ranges\u001b[0;34m(refdes, variable, api_user, api_token)\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0mglobal_max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'uFrame is not responding to request for global ranges. Try again later.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mglobal_min\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_max\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: uFrame is not responding to request for global ranges. Try again later."
     ]
    }
   ],
   "source": [
    "# calculate mean pressure from data, excluding outliers +/- 3 SD\n",
    "try:\n",
    "    pressure = ds[press]\n",
    "    num_dims = len(pressure.dims)\n",
    "    if len(pressure) > 1:\n",
    "        # reject NaNs\n",
    "        p_nonan = pressure.values[~np.isnan(pressure.values)]\n",
    "\n",
    "        # reject fill values\n",
    "        p_nonan_nofv = p_nonan[p_nonan != pressure._FillValue]\n",
    "\n",
    "        # reject data outside of global ranges\n",
    "        [pg_min, pg_max] = cf.get_global_ranges(rd, press)\n",
    "        if pg_min is not None and pg_max is not None:\n",
    "            try:\n",
    "                pgr_ind = cf.reject_global_ranges(p_nonan_nofv, pg_min, pg_max)\n",
    "                p_nonan_nofv_gr = p_nonan_nofv[pgr_ind]\n",
    "            except Exception: \n",
    "                print('uFrame is not responding to request for global ranges. Try again later.')\n",
    "                p_nonan_nofv_gr = p_nonan_nofv\n",
    "        else:\n",
    "            p_nonan_nofv_gr = p_nonan_nofv\n",
    "\n",
    "        if (len(p_nonan_nofv_gr) > 0) and (num_dims == 1):\n",
    "            [press_outliers, pressure_mean, _, pressure_max, _, _] = cf.variable_statistics(p_nonan_nofv_gr, 3)\n",
    "            pressure_mean = round(pressure_mean, 2)\n",
    "            pressure_max = round(pressure_max, 2)\n",
    "        elif (len(p_nonan_nofv_gr) > 0) and (num_dims > 1):\n",
    "            print('variable has more than 1 dimension')\n",
    "            press_outliers = 'not calculated: variable has more than 1 dimension'\n",
    "            pressure_mean = round(np.nanmean(p_nonan_nofv_gr), 2)\n",
    "            pressure_max = round(np.nanmax(p_nonan_nofv_gr), 2)\n",
    "        else:\n",
    "            press_outliers = None\n",
    "            pressure_mean = None\n",
    "            pressure_max = None\n",
    "            if len(pressure) > 0 and len(p_nonan) == 0:\n",
    "                notes.append('Pressure variable all NaNs')\n",
    "            elif len(pressure) > 0 and len(p_nonan) > 0 and len(p_nonan_nofv) == 0:\n",
    "                notes.append('Pressure variable all fill values')\n",
    "            elif len(pressure) > 0 and len(p_nonan) > 0 and len(p_nonan_nofv) > 0 and len(p_nonan_nofv_gr) == 0:\n",
    "                notes.append('Pressure variable outside of global ranges')\n",
    "\n",
    "    else:  # if there is only 1 data point\n",
    "        press_outliers = 0\n",
    "        pressure_mean = round(ds[press].values.tolist()[0], 2)\n",
    "        pressure_max = round(ds[press].values.tolist()[0], 2)\n",
    "\n",
    "    try:\n",
    "        pressure_units = pressure.units\n",
    "    except AttributeError:\n",
    "        pressure_units = 'no units attribute for pressure'\n",
    "\n",
    "    if pressure_mean:\n",
    "        node = refdes.split('-')[1]\n",
    "        if ('WFP' in node) or ('MOAS' in subsite):\n",
    "            pressure_compare = int(round(pressure_max))\n",
    "        else:\n",
    "            pressure_compare = int(round(pressure_mean))\n",
    "\n",
    "        if pressure_units == '0.001 dbar':\n",
    "            pressure_max = round((pressure_max / 1000), 2)\n",
    "            pressure_mean = round((pressure_mean / 1000), 2)\n",
    "            notes.append('Pressure converted from 0.001 dbar to dbar for pressure comparison')\n",
    "    else:\n",
    "        pressure_compare = None\n",
    "\n",
    "    if (not deploy_depth) or (not pressure_mean):\n",
    "        pressure_diff = None\n",
    "    else:\n",
    "        pressure_diff = pressure_compare - deploy_depth\n",
    "\n",
    "except KeyError:\n",
    "    press = 'no seawater pressure in file'\n",
    "    pressure_diff = None\n",
    "    pressure_mean = None\n",
    "    pressure_max = None\n",
    "    pressure_compare = None\n",
    "    press_outliers = None\n",
    "    pressure_units = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add files and info to dictionary\n",
    "filenames = data['deployments'][deployment]['method'][method]['stream'][data_stream][\n",
    "    'file'].keys()\n",
    "if fname not in filenames:\n",
    "    data['deployments'][deployment]['method'][method]['stream'][data_stream]['file'][\n",
    "        fname] = OrderedDict(\n",
    "        file_downloaded=pd.to_datetime(elements[0]).strftime('%Y-%m-%dT%H:%M:%S'),\n",
    "        file_coordinates=list(ds.coords.keys()),\n",
    "        sampling_rate_seconds=sampling_rt_sec,\n",
    "        sampling_rate_details=rates,\n",
    "        data_start=data_start,\n",
    "        data_stop=data_stop,\n",
    "        time_gaps=gap_list,\n",
    "        unique_timestamps=time_test,\n",
    "        n_timestamps=len_time,\n",
    "        n_days=n_days,\n",
    "        notes=notes,\n",
    "        ascending_timestamps=time_ascending,\n",
    "        pressure_comparison=dict(pressure_mean=pressure_mean, units=pressure_units,\n",
    "                                 num_outliers=press_outliers, diff=pressure_diff,\n",
    "                                 pressure_max=pressure_max, variable=press,\n",
    "                                 pressure_compare=pressure_compare),\n",
    "        vars_in_file=ds_variables,\n",
    "        vars_not_in_file=[x for x in unmatch1 if 'time' not in x],\n",
    "        vars_not_in_db=unmatch2,\n",
    "        sci_var_stats=OrderedDict())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
