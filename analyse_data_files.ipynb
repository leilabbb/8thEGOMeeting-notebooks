{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse OOI netCDF Files For An Instrument"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Input variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THERDD server contains the netCDF data files\n",
    "\n",
    "url_list = ['https://opendap.oceanobservatories.org/thredds/catalog/ooi/leila.ocean@gmail.com/20190306T174413-CP05MOAS-GL379-03-CTDGVM000-recovered_host-ctdgv_m_glider_instrument_recovered/catalog.html',\n",
    "'https://opendap.oceanobservatories.org/thredds/catalog/ooi/leila.ocean@gmail.com/20190306T174435-CP05MOAS-GL379-03-CTDGVM000-telemetered-ctdgv_m_glider_instrument/catalog.html']\n",
    "\n",
    "# url_list = ['https://opendap.oceanobservatories.org/thredds/catalog/ooi/leila.ocean@gmail.com/20190319T195519-CP05MOAS-GL335-03-CTDGVM000-recovered_host-ctdgv_m_glider_instrument_recovered/catalog.html',\n",
    "#             'https://opendap.oceanobservatories.org/thredds/catalog/ooi/leila.ocean@gmail.com/20190319T195533-CP05MOAS-GL335-03-CTDGVM000-telemetered-ctdgv_m_glider_instrument/catalog.html'] \n",
    "\n",
    "# review file was created upfront to do this analysis\n",
    "review_file = 'https://raw.githubusercontent.com/ooi-data-lab/data-review-prep/master/review_list/data_review_list.csv'\n",
    "\n",
    "# f =  #location to a file containing THREDDs urls with .nc files to analyze. \n",
    "#The column containing the THREDDs urls must be labeled 'outputUrl'\n",
    "\n",
    "# Define folder path to save summary output\n",
    "sDir =  '/Users/leila/Documents/NSFEduSupport/review/output'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import json\n",
    "import datetime as dt\n",
    "import netCDF4 as nc\n",
    "import functions.common as cf\n",
    "import functions.plotting as pf\n",
    "from datetime import timedelta\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Data Files With Status For Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data request has fulfilled.\n",
      "Data request has fulfilled.\n",
      "CP05MOAS-GL379-03-CTDGVM000 : number of files in THREDDS ==  7\n"
     ]
    }
   ],
   "source": [
    "reviewlist = pd.read_csv(review_file)\n",
    "datasets = []\n",
    "for uu in url_list:\n",
    "    # get instrument  = reference designator \n",
    "    elements = uu.split('/')[-2].split('-')\n",
    "    rd = '-'.join((elements[1], elements[2], elements[3], elements[4]))\n",
    "    \n",
    "    data = OrderedDict(deployments=OrderedDict())\n",
    "    \n",
    "    # create an output file\n",
    "    save_dir = os.path.join(sDir, rd.split('-')[0], rd)\n",
    "    cf.create_dir(save_dir)\n",
    "    \n",
    "    # check for the OOI 1.0 datasets for review    \n",
    "    rl_filtered = reviewlist.loc[(reviewlist['Reference Designator'] == rd) & (reviewlist['status'] == 'for review')]\n",
    "    \n",
    "    # print to the screen\n",
    "    catalog_rms = '-'.join((rd, elements[-2], elements[-1]))\n",
    "#    print(catalog_rms)\n",
    "#    print(pd.DataFrame({'deploymentNumber': rl_filtered['deploymentNumber'],\n",
    "#                        'startDateTime': rl_filtered['startDateTime'],\n",
    "#                        'stopDateTime': rl_filtered['stopDateTime'],\n",
    "#                        'in_am': rl_filtered['in_am']}))\n",
    "    \n",
    "        \n",
    "    # get data files from THREDDS server\n",
    "    udatasets = cf.get_nc_urls([uu])\n",
    "    \n",
    "    # get deployments from file names\n",
    "    review_deployments = rl_filtered['deploymentNumber'].tolist()\n",
    "    review_deployments_int = ['deployment%04d' % int(x) for x in review_deployments]\n",
    "    \n",
    "    # get data files of interest\n",
    "    for rev_dep in review_deployments_int:\n",
    "        rdatasets = [s for s in udatasets if rev_dep in s]\n",
    "        if len(rdatasets) > 0:            \n",
    "            for dss in rdatasets:  # filter out collocated data files\n",
    "                if catalog_rms == dss.split('/')[-1].split('_20')[0][15:]:\n",
    "                    datasets.append(dss)\n",
    "print(rd, ': number of files in THREDDS == ',len(datasets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dr_data = cf.refdes_datareview_json(rd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a data frame with data files information\n",
    "df = pd.DataFrame({'refdes': [],\n",
    "                    'method': [],\n",
    "                    'data_stream':[],\n",
    "                    'n_days_deployed': [],\n",
    "                    'n_days_file': [],\n",
    "                    'num_timestamps': [],                        \n",
    "                    'sampling_rt_sec': [],\n",
    "                    'time_order': [],\n",
    "                    'gap_list': [],\n",
    "                    'num_gaps': [],                      \n",
    "                    'pressure_comp': [],\n",
    "                    'coord_test': [],\n",
    "                    'datasets': []})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ii, fname in enumerate(datasets):    \n",
    "    refdes = fname.split('/')[-1].split('_')[1][0:27]\n",
    "    deploy_num = int(fname.split('/')[-1].split('-')[0][13:-9])\n",
    "    deploy_info = cf.get_deployment_information(dr_data, deploy_num)\n",
    "    deploy_depth = deploy_info['deployment_depth']\n",
    "    \n",
    "    # Calculate days deployed\n",
    "    deploy_start = str(deploy_info['start_date'])\n",
    "    deploy_stop = str(deploy_info['stop_date'])    \n",
    "    if deploy_stop != 'None':\n",
    "        r_deploy_start = pd.to_datetime(deploy_start).replace(hour=0, minute=0, second=0)\n",
    "        if deploy_stop.split('T')[1] == '00:00:00':\n",
    "            r_deploy_stop = pd.to_datetime(deploy_stop)\n",
    "        else:\n",
    "            r_deploy_stop = (pd.to_datetime(deploy_stop) + timedelta(days=1)).replace(hour=0, minute=0, second=0)\n",
    "        n_days_deployed = (r_deploy_stop - r_deploy_start).days\n",
    "    else:\n",
    "        n_days_deployed = None\n",
    "    \n",
    "    \n",
    "    # Get time array\n",
    "    ds = xr.open_dataset(fname, mask_and_scale=False)\n",
    "    ds = ds.swap_dims({'obs': 'time'})\n",
    "    time = ds['time']\n",
    "\n",
    "    # Check that the timestamps in the file are unique\n",
    "    len_time = time.__len__()\n",
    "    len_time_unique = np.unique(time).__len__()\n",
    "    if len_time == len_time_unique:\n",
    "        time_unique = 'pass'\n",
    "    else:\n",
    "        time_unique = 'fail'\n",
    "     \n",
    "    # Check that the timestamps in the file are in ascending order\n",
    "    # convert time to number\n",
    "    time_in = [dt.datetime.utcfromtimestamp(np.datetime64(x).astype('O')/1e9) for x in time.values]\n",
    "    time_data = nc.date2num(time_in, 'seconds since 1900-01-01')\n",
    "\n",
    "    # Create a list of True or False by iterating through the array of time and checking\n",
    "    # if every time stamp is increasing\n",
    "    result = [(time_data[k + 1] - time_data[k]) > 0 for k in range(len(time_data) - 1)]\n",
    "\n",
    "    # Print outcome of the iteration with the list of indices when time is not increasing\n",
    "    if result.count(True) == len(time) - 1:\n",
    "        time_ascending = 'pass'\n",
    "    else:\n",
    "        ind_fail = {k: time_in[k] for k, v in enumerate(result) if v is False}\n",
    "        time_ascending = 'fail: {}'.format(ind_fail)\n",
    "        \n",
    "    # Count the number of days for which there is at least 1 timestamp    \n",
    "    n_days = len(np.unique(time.values.astype('datetime64[D]')))\n",
    "    \n",
    "    # Get a list of data gaps >1 day    \n",
    "    time_df = pd.DataFrame(time.values, columns=['time'])\n",
    "    gap_list = cf.timestamp_gap_test(time_df)\n",
    "        \n",
    "    # Calculate the sampling rate to the nearest second\n",
    "    time_df['diff'] = time_df['time'].diff().astype('timedelta64[s]')\n",
    "    rates_df = time_df.groupby(['diff']).agg(['count'])\n",
    "    n_diff_calc = len(time_df) - 1\n",
    "    rates = dict(n_unique_rates=len(rates_df), common_sampling_rates=dict())\n",
    "    for i, row in rates_df.iterrows():\n",
    "        percent = (float(row['time']['count']) / float(n_diff_calc))\n",
    "        if percent > 0.1:\n",
    "            rates['common_sampling_rates'].update({int(i): '{:.2%}'.format(percent)})\n",
    "    sampling_rt_sec = None\n",
    "    for k, v in rates['common_sampling_rates'].items():\n",
    "        if float(v.strip('%')) > 50.00:\n",
    "            sampling_rt_sec = k\n",
    "\n",
    "    if not sampling_rt_sec:\n",
    "        sampling_rt_sec = 'no consistent sampling rate: {}'.format(rates['common_sampling_rates']) \n",
    "        \n",
    "    # Check deployment pressure from asset management against pressure variable in file\n",
    "    press = pf.pressure_var(ds, list(ds.coords.keys()))\n",
    "    if press is None:\n",
    "        press = pf.pressure_var(ds, list(ds.data_vars.keys()))     \n",
    "    pressure_compare, pressure_max, pressure_mean = cf.calculate_mean_pressure(press, ds, refdes, deploy_depth)\n",
    "        \n",
    "    # check coordinate\n",
    "    file_coordinates = list(ds.coords.keys())\n",
    "    if 'SBD' not in refdes.split('-')[1]:\n",
    "        check_coords = list(set(['obs', 'time', 'pressure', 'lat', 'lon']) - set(file_coordinates))\n",
    "    else:\n",
    "        check_coords = list(set(['obs', 'time', 'lat', 'lon']) - set(file_coordinates))\n",
    "    \n",
    "    if len(check_coords) > 0:\n",
    "        if 'pressure' in check_coords:\n",
    "            if len([j for j in file_coordinates if 'pressure' in j]) == 1:\n",
    "                check_coords.remove('pressure')\n",
    "                if len(check_coords) > 0:\n",
    "                    coord_test = 'missing: {}'.format(check_coords)\n",
    "                else:\n",
    "                    coord_test = 'pass'\n",
    "            else:\n",
    "                coord_test = 'missing: {}'.format(check_coords)\n",
    "        else:\n",
    "            coord_test = 'missing: {}'.format(check_coords)\n",
    "    else:\n",
    "        coord_test = 'pass'\n",
    "    \n",
    "#         data_start=data_start,\n",
    "#         data_stop=data_stop,\n",
    "#         notes=notes,       \n",
    "#         units=pressure_units,\n",
    "#         variable=press\n",
    "#         vars_in_file=ds_variables,\n",
    "#         vars_not_in_file=[x for x in unmatch1 if 'time' not in x],\n",
    "#         vars_not_in_db=unmatch2,\n",
    "#         sci_var_stats=OrderedDict())\n",
    "        \n",
    "    df0 = pd.DataFrame({'refdes': [refdes],\n",
    "                        'method': [fname.split('/')[-1].split('-')[4]],\n",
    "                        'data_stream':[fname.split('/')[-1].split('-')[5][:-23]],\n",
    "                        'n_days_deployed': [n_days_deployed],\n",
    "                        'n_days_file': [n_days],\n",
    "                        'num_timestamps': [len_time],                        \n",
    "                        'sampling_rt_sec': [sampling_rt_sec],\n",
    "                        'time_order': [['Unique-'+time_unique,'Ascending-'+time_ascending]],\n",
    "                        'gap_list': [gap_list],\n",
    "                        'num_gaps': [int(len(gap_list))],                      \n",
    "                        'pressure_comp': [[deploy_depth, pressure_max]],\n",
    "                        'coord_test': [coord_test],\n",
    "                        'datasets': [fname]\n",
    "                        }, index=[deploy_num])\n",
    "    df = df.append(df0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "method_list = ['streamed', 'recovered_inst', 'recovered_wfp', 'recovered_cspp', 'recovered_host', 'telemetered']\n",
    "deployments = np.unique(np.sort(list(df.index.values)))\n",
    "df_info = pd.DataFrame()\n",
    "for d in deployments:\n",
    "    df_d = df[df.index.values == d]\n",
    "    if len(df_d['method']) != 1: # sort methods in order of preference\n",
    "        z = sorted(df_d['method'], key=lambda zz: method_list.index(zz)) # sorted method list        \n",
    "        df_d = df_d[df_d['method'] == z[0]]\n",
    "\n",
    "    df_info = df_info.append(df_d)\n",
    "len(df_info.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "practical_salinity 1518 35.3557 33.0998 36.5786 0.4433 1696159 1 4 0 -9999999.0 0 99.91 {'99': 1} []\n",
      "sci_water_cond 1518 4.1648 3.2019 5.5363 0.5035 1696159 S m-1 0 0 nan 4 99.91 {'99': 2} []\n",
      "sci_water_pressure 0 18.5088 0.0 101.11 21.5955 1697681 bar 0 0 nan 0 100.0 {'99': 3} []\n",
      "sci_water_temp 0 13.0989 0.0 25.4908 4.8591 1697681 ºC 0 0 nan 0 100.0 {'99': 4} []\n",
      "sci_seawater_density 0 1027.3765 1023.3115 1032.3609 1.6905 1696159 kg m-3 4 0 nan 1518 99.91 {'99': 5} []\n",
      "sci_water_pressure_dbar 0 185.0881 0.0 1011.1 215.9547 1697681 dbar 0 0 nan 0 100.0 {'99': 6} []\n",
      "{'99': 6}\n",
      "practical_salinity 639 35.2059 31.5974 38.8005 0.3742 1875850 1 9 0 -9999999.0 0 99.97 {'99': 1} []\n",
      "sci_water_cond 577 3.7036 3.3225 5.5537 0.4065 1875912 S m-1 0 0 nan 9 99.97 {'99': 2} []\n",
      "sci_water_pressure 0 47.5734 0.0 98.627 28.2113 1876498 bar 0 0 nan 0 100.0 {'99': 3} []\n",
      "sci_water_temp 0 8.3477 0.0 26.0479 4.0622 1876498 ºC 0 0 nan 0 100.0 {'99': 4} []\n",
      "sci_seawater_density 0 1029.4759 1021.0847 1032.2592 1.7614 1875912 kg m-3 9 0 nan 577 99.97 {'99': 5} []\n",
      "sci_water_pressure_dbar 0 475.7337 0.0 986.27 282.1129 1876498 dbar 0 0 nan 0 100.0 {'99': 6} []\n",
      "{'99': 6}\n",
      "practical_salinity 1343 35.1847 33.3491 36.1406 0.306 3821445 1 0 0 -9999999.0 0 99.96 {'99': 1} []\n",
      "sci_water_cond 1339 3.698 3.3161 4.5442 0.355 3821449 S m-1 0 0 nan 0 99.96 {'99': 2} []\n",
      "sci_water_pressure 0 43.0017 0.0 96.127 27.3051 3822788 bar 0 0 nan 0 100.0 {'99': 3} []\n",
      "sci_water_temp 0 8.3505 0.0 16.4964 3.6128 3822788 ºC 0 0 nan 0 100.0 {'99': 4} []\n",
      "sci_seawater_density 1 1029.2669 1023.2164 1032.1469 1.6572 3821448 kg m-3 0 0 nan 1339 99.96 {'99': 5} []\n",
      "sci_water_pressure_dbar 0 430.0173 0.0 961.27 273.0508 3822788 dbar 0 0 nan 0 100.0 {'99': 6} []\n",
      "{'99': 6}\n",
      "practical_salinity 0 31.6605 0.0 35.8872 11.0452 212 1 0 0 -9999999.0 0 100.0 {'99': 1} []\n",
      "sci_water_cond 0 3.8097 0.0 4.4162 1.3309 212 S m-1 0 0 nan 0 100.0 {'99': 2} []\n",
      "sci_water_pressure 0 5.8828 0.0 26.165 7.4923 212 bar 0 0 nan 0 100.0 {'99': 3} []\n",
      "sci_water_temp 0 12.7058 0.0 15.256 4.4025 212 ºC 0 0 nan 0 100.0 {'99': 4} []\n",
      "sci_seawater_density 0 1026.8335 1026.429 1028.2291 0.509 189 kg m-3 0 0 nan 23 89.15 {'99': 4, '75': 1} [4]\n",
      "sci_water_pressure_dbar 0 58.8284 0.0 261.65 74.9231 212 dbar 0 0 nan 0 100.0 {'99': 5, '75': 1} [4]\n",
      "{'99': 5, '75': 1}\n"
     ]
    }
   ],
   "source": [
    "valid_list = list(np.zeros(len(df_info)))\n",
    "for index, row in df_info.iterrows():\n",
    "    sci_vars = cf.return_science_vars(row['data_stream'])\n",
    "    ds = xr.open_dataset(row['datasets'], mask_and_scale=False)\n",
    "    ds = ds.swap_dims({'obs': 'time'})\n",
    "    # calculate statistics for science variables, excluding outliers +/- 5 SD\n",
    "    valid_list_index = []\n",
    "    for sv in sci_vars:\n",
    "        try:\n",
    "            var = ds[sv]\n",
    "            num_dims = len(var.dims)\n",
    "\n",
    "            if num_dims > 1:\n",
    "                print('variable has more than 1 dimension')\n",
    "                n_all =  None\n",
    "                num_outliers = None\n",
    "                mean = None\n",
    "                vmin = None\n",
    "                vmax = None\n",
    "                sd = None\n",
    "                n_stats = 'variable has more than 1 dimension'\n",
    "                var_units = var.units\n",
    "                n_nan = None\n",
    "                n_fv = None\n",
    "                n_grange = None\n",
    "                fv = None\n",
    "            else:\n",
    "                n_all =  len(var)\n",
    "                # reject NaNs\n",
    "                var_nonan = var.values[~np.isnan(var.values)]\n",
    "                n_nan = len(var) - len(var_nonan)\n",
    "\n",
    "                # reject fill values\n",
    "                fv = var._FillValue\n",
    "                var_nonan_nofv = var_nonan[var_nonan != fv]\n",
    "                n_fv = len(var) - n_nan - len(var_nonan_nofv)\n",
    "\n",
    "                # reject data outside of global ranges\n",
    "                try: \n",
    "                    [g_min, g_max] = cf.get_global_ranges(rd, sv)\n",
    "                    if g_min is not None and g_max is not None:\n",
    "                        gr_ind = cf.reject_global_ranges(var_nonan_nofv, g_min, g_max)\n",
    "                        var_nonan_nofv_gr = var_nonan_nofv[gr_ind]\n",
    "                        n_grange = len(var) - n_nan - n_fv - len(var_nonan_nofv_gr)\n",
    "                    else:\n",
    "                        n_grange = 'no global ranges'\n",
    "                        var_nonan_nofv_gr = var_nonan_nofv\n",
    "                except Exception:\n",
    "                    print('uFrame is not responding to request for global ranges. Try again later.')\n",
    "                    var_nonan_nofv_gr = var_nonan_nofv\n",
    "\n",
    "                if len(var_nonan_nofv_gr) > 1:\n",
    "                    [num_outliers, mean, vmin, vmax, sd, n_stats] = cf.variable_statistics(var_nonan_nofv_gr, 5)\n",
    "                elif len(var_nonan_nofv_gr) == 1:\n",
    "                    num_outliers = 0\n",
    "                    mean = (round(list(var_nonan_nofv_gr)[0], 4)).astype('float64')\n",
    "                    vmin = None\n",
    "                    vmax = None\n",
    "                    sd = None\n",
    "                    n_stats = 1\n",
    "                else:\n",
    "                    num_outliers = None\n",
    "                    mean = None\n",
    "                    vmin = None\n",
    "                    vmax = None\n",
    "                    sd = None\n",
    "                    n_stats = 0\n",
    "\n",
    "                var_units = var.units\n",
    "\n",
    "        except KeyError:\n",
    "            n_all =  None\n",
    "            num_outliers = None\n",
    "            mean = None\n",
    "            vmin = None\n",
    "            vmax = None\n",
    "            sd = None\n",
    "            n_stats = 'variable not found in file'\n",
    "            var_units = None\n",
    "            n_nan = None\n",
    "            n_fv = None\n",
    "            fv = None\n",
    "            n_grange = None\n",
    "            \n",
    "        if type(n_stats) == str:\n",
    "            percent_valid_data = 'stats not calculated'\n",
    "        elif type(n_all) == list:\n",
    "            if type(n_gr) == str:\n",
    "                n1 = n_all[1] - n_nan - n_fv\n",
    "            else:\n",
    "                n1 = n_all[1] - n_nan - n_fv - n_gr\n",
    "            percent_valid_data = round((float(n1) / float(n_all[1]) * 100), 2)\n",
    "        else:\n",
    "            percent_valid_data = round((float(n_stats)/float(n_all) * 100), 2)\n",
    "        valid_list_index.append(percent_valid_data)\n",
    "        \n",
    "        pvd_test = dict()\n",
    "        snc = len([x for x in valid_list_index if x == 'stats not calculated'])\n",
    "        if snc > 0:\n",
    "            pvd_test['stats not calculated'] = snc\n",
    "        else:\n",
    "            valid_list_index = [round(v) for v in valid_list_index]\n",
    "            pvd_test, dlst = cf.group_percents(pvd_test, valid_list_index)\n",
    "        print(sv, num_outliers, mean, vmin, vmax, sd, n_stats, var_units, n_nan, n_fv, fv, n_grange, percent_valid_data, pvd_test, dlst)\n",
    "    \n",
    "    valid_list[index-1] = pvd_test                            \n",
    "    print(valid_list[index-1])    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>refdes</th>\n",
       "      <th>method</th>\n",
       "      <th>data_stream</th>\n",
       "      <th>n_days_deployed</th>\n",
       "      <th>n_days_file</th>\n",
       "      <th>num_timestamps</th>\n",
       "      <th>sampling_rt_sec</th>\n",
       "      <th>time_order</th>\n",
       "      <th>gap_list</th>\n",
       "      <th>num_gaps</th>\n",
       "      <th>pressure_comp</th>\n",
       "      <th>coord_test</th>\n",
       "      <th>datasets</th>\n",
       "      <th>valid_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CP05MOAS-GL379-03-CTDGVM000</td>\n",
       "      <td>recovered_host</td>\n",
       "      <td>ctdgv_m_glider_instrument_recovered</td>\n",
       "      <td>88.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>1697681.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>[Unique-pass, Ascending-pass]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[1000, 832.95]</td>\n",
       "      <td>pass</td>\n",
       "      <td>https://opendap.oceanobservatories.org/thredds...</td>\n",
       "      <td>{'99': 6}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CP05MOAS-GL379-03-CTDGVM000</td>\n",
       "      <td>recovered_host</td>\n",
       "      <td>ctdgv_m_glider_instrument_recovered</td>\n",
       "      <td>55.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>1876498.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>[Unique-pass, Ascending-pass]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[1000, 986.27]</td>\n",
       "      <td>pass</td>\n",
       "      <td>https://opendap.oceanobservatories.org/thredds...</td>\n",
       "      <td>{'99': 6}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CP05MOAS-GL379-03-CTDGVM000</td>\n",
       "      <td>recovered_host</td>\n",
       "      <td>ctdgv_m_glider_instrument_recovered</td>\n",
       "      <td>119.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>3822788.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>[Unique-pass, Ascending-pass]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[1000, 961.27]</td>\n",
       "      <td>pass</td>\n",
       "      <td>https://opendap.oceanobservatories.org/thredds...</td>\n",
       "      <td>{'99': 6}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CP05MOAS-GL379-03-CTDGVM000</td>\n",
       "      <td>telemetered</td>\n",
       "      <td>ctdgv_m_glider_instrument</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>212.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>[Unique-pass, Ascending-pass]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[1000, 261.65]</td>\n",
       "      <td>pass</td>\n",
       "      <td>https://opendap.oceanobservatories.org/thredds...</td>\n",
       "      <td>{'99': 5, '75': 1}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        refdes          method  \\\n",
       "1  CP05MOAS-GL379-03-CTDGVM000  recovered_host   \n",
       "2  CP05MOAS-GL379-03-CTDGVM000  recovered_host   \n",
       "3  CP05MOAS-GL379-03-CTDGVM000  recovered_host   \n",
       "4  CP05MOAS-GL379-03-CTDGVM000     telemetered   \n",
       "\n",
       "                           data_stream  n_days_deployed  n_days_file  \\\n",
       "1  ctdgv_m_glider_instrument_recovered             88.0         83.0   \n",
       "2  ctdgv_m_glider_instrument_recovered             55.0         48.0   \n",
       "3  ctdgv_m_glider_instrument_recovered            119.0        119.0   \n",
       "4            ctdgv_m_glider_instrument              1.0          1.0   \n",
       "\n",
       "   num_timestamps  sampling_rt_sec                     time_order gap_list  \\\n",
       "1       1697681.0              2.0  [Unique-pass, Ascending-pass]       []   \n",
       "2       1876498.0              2.0  [Unique-pass, Ascending-pass]       []   \n",
       "3       3822788.0              2.0  [Unique-pass, Ascending-pass]       []   \n",
       "4           212.0             60.0  [Unique-pass, Ascending-pass]       []   \n",
       "\n",
       "   num_gaps   pressure_comp coord_test  \\\n",
       "1       0.0  [1000, 832.95]       pass   \n",
       "2       0.0  [1000, 986.27]       pass   \n",
       "3       0.0  [1000, 961.27]       pass   \n",
       "4       0.0  [1000, 261.65]       pass   \n",
       "\n",
       "                                            datasets          valid_data  \n",
       "1  https://opendap.oceanobservatories.org/thredds...           {'99': 6}  \n",
       "2  https://opendap.oceanobservatories.org/thredds...           {'99': 6}  \n",
       "3  https://opendap.oceanobservatories.org/thredds...           {'99': 6}  \n",
       "4  https://opendap.oceanobservatories.org/thredds...  {'99': 5, '75': 1}  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_info.insert(len(df_info.columns), column='valid_data', value=valid_list)\n",
    "df_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-110-f535d749d2a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mii\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0msci_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_science_vars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data_stream'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mii\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mii\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_and_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswap_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'obs'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'time'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# calculate statistics for science variables, excluding outliers +/- 5 SD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    765\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 767\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self, series, key)\u001b[0m\n\u001b[1;32m   3116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3117\u001b[0m             return self._engine.get_value(s, k,\n\u001b[0;32m-> 3118\u001b[0;31m                                           tz=getattr(series.dtype, 'tz', None))\n\u001b[0m\u001b[1;32m   3119\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3120\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minferred_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'integer'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'boolean'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "# for i, row in enumerate(df_info.values):\n",
    "#     sci_vars = cf.return_science_vars(row['data_stream'])\n",
    "#     ds = xr.open_dataset(row['datasets'], mask_and_scale=False)\n",
    "#     ds = ds.swap_dims({'obs': 'time'})\n",
    "#     # calculate statistics for science variables, excluding outliers +/- 5 SD\n",
    "#     for sv in sci_vars:\n",
    "#         print(sv)\n",
    "#         try:\n",
    "#             var = ds[sv]\n",
    "#             num_dims = len(var.dims)\n",
    "\n",
    "#             if num_dims > 1:\n",
    "#                 print('variable has more than 1 dimension')\n",
    "#                 num_outliers = None\n",
    "#                 mean = None\n",
    "#                 vmin = None\n",
    "#                 vmax = None\n",
    "#                 sd = None\n",
    "#                 n_stats = 'variable has more than 1 dimension'\n",
    "#                 var_units = var.units\n",
    "#                 n_nan = None\n",
    "#                 n_fv = None\n",
    "#                 n_grange = None\n",
    "#                 fv = None\n",
    "#             else:\n",
    "#                 # reject NaNs\n",
    "#                 var_nonan = var.values[~np.isnan(var.values)]\n",
    "#                 n_nan = len(var) - len(var_nonan)\n",
    "\n",
    "#                 # reject fill values\n",
    "#                 fv = var._FillValue\n",
    "#                 var_nonan_nofv = var_nonan[var_nonan != fv]\n",
    "#                 n_fv = len(var) - n_nan - len(var_nonan_nofv)\n",
    "\n",
    "#                 # reject data outside of global ranges\n",
    "#                 try: \n",
    "#                     [g_min, g_max] = cf.get_global_ranges(rd, sv)\n",
    "#                     if g_min is not None and g_max is not None:\n",
    "#                         gr_ind = cf.reject_global_ranges(var_nonan_nofv, g_min, g_max)\n",
    "#                         var_nonan_nofv_gr = var_nonan_nofv[gr_ind]\n",
    "#                         n_grange = len(var) - n_nan - n_fv - len(var_nonan_nofv_gr)\n",
    "#                     else:\n",
    "#                         n_grange = 'no global ranges'\n",
    "#                         var_nonan_nofv_gr = var_nonan_nofv\n",
    "#                 except Exception:\n",
    "#                     print('uFrame is not responding to request for global ranges. Try again later.')\n",
    "#                     var_nonan_nofv_gr = var_nonan_nofv\n",
    "\n",
    "#                 if len(var_nonan_nofv_gr) > 1:\n",
    "#                     [num_outliers, mean, vmin, vmax, sd, n_stats] = cf.variable_statistics(var_nonan_nofv_gr, 5)\n",
    "#                 elif len(var_nonan_nofv_gr) == 1:\n",
    "#                     num_outliers = 0\n",
    "#                     mean = (round(list(var_nonan_nofv_gr)[0], 4)).astype('float64')\n",
    "#                     vmin = None\n",
    "#                     vmax = None\n",
    "#                     sd = None\n",
    "#                     n_stats = 1\n",
    "#                 else:\n",
    "#                     num_outliers = None\n",
    "#                     mean = None\n",
    "#                     vmin = None\n",
    "#                     vmax = None\n",
    "#                     sd = None\n",
    "#                     n_stats = 0\n",
    "\n",
    "#                 var_units = var.units\n",
    "\n",
    "#         except KeyError:\n",
    "#             num_outliers = None\n",
    "#             mean = None\n",
    "#             vmin = None\n",
    "#             vmax = None\n",
    "#             sd = None\n",
    "#             n_stats = 'variable not found in file'\n",
    "#             var_units = None\n",
    "#             n_nan = None\n",
    "#             n_fv = None\n",
    "#             fv = None\n",
    "#             n_grange = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # read data file\n",
    "# notes = []\n",
    "# time_ascending = ''\n",
    "# if len(datasets) == 1:\n",
    "#     ds = xr.open_dataset(datasets[0], mask_and_scale=False)\n",
    "#     ds = ds.swap_dims({'obs': 'time'})\n",
    "# #     fname, subsite, refdes, method, data_stream, deployment = cf.nc_attributes(datasets[0])\n",
    "# elif len(datasets) > 1:\n",
    "#     ds = xr.open_mfdataset(datasets, mask_and_scale=False)\n",
    "#     ds = ds.swap_dims({'obs': 'time'})\n",
    "#     ds = ds.chunk({'time': 100})\n",
    "# #     fname, subsite, refdes, method, data_stream, deployment = cf.nc_attributes(datasets[0])\n",
    "# #     fname = fname.split('_20')[0]\n",
    "#     notes.append('multiple deployment .nc files')\n",
    "#     # when opening multiple datasets, don't check that the timestamps are in ascending order\n",
    "#     time_ascending = 'not_tested'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  # Get info from the data review database\n",
    "# dr_data = cf.refdes_datareview_json(refdes)\n",
    "# stream_vars = cf.return_stream_vars(data_stream)\n",
    "# sci_vars = cf.return_science_vars(data_stream)\n",
    "# deploy_info = cf.get_deployment_information(dr_data, int(deployment[-4:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab deployment Variables\n",
    "# deploy_start = str(deploy_info['start_date'])\n",
    "# deploy_stop = str(deploy_info['stop_date'])\n",
    "# deploy_lon = deploy_info['longitude']\n",
    "# deploy_lat = deploy_info['latitude']\n",
    "# deploy_depth = deploy_info['deployment_depth']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate days deployed\n",
    "# if deploy_stop != 'None':\n",
    "#     r_deploy_start = pd.to_datetime(deploy_start).replace(hour=0, minute=0, second=0)\n",
    "#     if deploy_stop.split('T')[1] == '00:00:00':\n",
    "#         r_deploy_stop = pd.to_datetime(deploy_stop)\n",
    "#     else:\n",
    "#         r_deploy_stop = (pd.to_datetime(deploy_stop) + timedelta(days=1)).replace(hour=0, minute=0, second=0)\n",
    "#     n_days_deployed = (r_deploy_stop - r_deploy_start).days\n",
    "# else:\n",
    "#     n_days_deployed = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add reference designator to dictionary\n",
    "try:\n",
    "    data['refdes']\n",
    "except KeyError:\n",
    "    data['refdes'] = refdes\n",
    "\n",
    "deployments = data['deployments'].keys()\n",
    "data_start = pd.to_datetime(min(ds['time'].values)).strftime('%Y-%m-%dT%H:%M:%S')\n",
    "data_stop = pd.to_datetime(max(ds['time'].values)).strftime('%Y-%m-%dT%H:%M:%S')\n",
    "\n",
    "# Add deployment and info to dictionary and initialize delivery method sub-dictionary\n",
    "if deployment not in deployments:\n",
    "    data['deployments'][deployment] = OrderedDict(deploy_start=deploy_start,\n",
    "                                                  deploy_stop=deploy_stop,\n",
    "                                                  n_days_deployed=n_days_deployed,\n",
    "                                                  lon=deploy_lon,\n",
    "                                                  lat=deploy_lat,\n",
    "                                                  deploy_depth=deploy_depth,\n",
    "                                                  method=OrderedDict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add delivery methods to dictionary and initialize stream sub-dictionary\n",
    "methods = data['deployments'][deployment]['method'].keys()\n",
    "if method not in methods:\n",
    "    data['deployments'][deployment]['method'][method] = OrderedDict(stream=OrderedDict())\n",
    "    \n",
    "# Add streams to dictionary and initialize file sub-dictionary\n",
    "streams = data['deployments'][deployment]['method'][method]['stream'].keys()\n",
    "if data_stream not in streams:\n",
    "    data['deployments'][deployment]['method'][method]['stream'][data_stream] = OrderedDict(file=OrderedDict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get a list of data gaps >1 day\n",
    "# time_df = pd.DataFrame(ds['time'].values, columns=['time'])\n",
    "# gap_list = cf.timestamp_gap_test(time_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate the sampling rate to the nearest second\n",
    "# time_df['diff'] = time_df['time'].diff().astype('timedelta64[s]')\n",
    "# rates_df = time_df.groupby(['diff']).agg(['count'])\n",
    "# n_diff_calc = len(time_df) - 1\n",
    "# rates = dict(n_unique_rates=len(rates_df), common_sampling_rates=dict())\n",
    "# for i, row in rates_df.iterrows():\n",
    "#     percent = (float(row['time']['count']) / float(n_diff_calc))\n",
    "#     if percent > 0.1:\n",
    "#         rates['common_sampling_rates'].update({int(i): '{:.2%}'.format(percent)})\n",
    "# sampling_rt_sec = None\n",
    "# for k, v in rates['common_sampling_rates'].items():\n",
    "#     if float(v.strip('%')) > 50.00:\n",
    "#         sampling_rt_sec = k\n",
    "\n",
    "# if not sampling_rt_sec:\n",
    "#     sampling_rt_sec = 'no consistent sampling rate: {}'.format(rates['common_sampling_rates'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check that the timestamps in the file are unique\n",
    "# time = ds['time']\n",
    "# len_time = time.__len__()\n",
    "# len_time_unique = np.unique(time).__len__()\n",
    "# if len_time == len_time_unique:\n",
    "#     time_test = 'pass'\n",
    "# else:\n",
    "#     time_test = 'fail'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check that the timestamps in the file are in ascending order\n",
    "# if time_ascending != 'not_tested':\n",
    "#     # convert time to number\n",
    "#     time_in = [dt.datetime.utcfromtimestamp(np.datetime64(x).astype('O')/1e9) for x in\n",
    "#                ds['time'].values]\n",
    "#     time_data = nc.date2num(time_in, 'seconds since 1900-01-01')\n",
    "\n",
    "#     # Create a list of True or False by iterating through the array of time and checking\n",
    "#     # if every time stamp is increasing\n",
    "#     result = [(time_data[k + 1] - time_data[k]) > 0 for k in range(len(time_data) - 1)]\n",
    "\n",
    "#     # Print outcome of the iteration with the list of indices when time is not increasing\n",
    "#     if result.count(True) == len(time) - 1:\n",
    "#         time_ascending = 'pass'\n",
    "#     else:\n",
    "#         ind_fail = {k: time_in[k] for k, v in enumerate(result) if v is False}\n",
    "#         time_ascending = 'fail: {}'.format(ind_fail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Count the number of days for which there is at least 1 timestamp\n",
    "# n_days = len(np.unique(time.values.astype('datetime64[D]')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare variables in file to variables in Data Review Database\n",
    "ds_variables = list(ds.data_vars.keys()) + list(ds.coords.keys())\n",
    "#ds_variables = [k for k in ds]\n",
    "ds_variables = cf.eliminate_common_variables(ds_variables)\n",
    "ds_variables = [x for x in ds_variables if 'qc' not in x]\n",
    "[_, unmatch1] = cf.compare_lists(stream_vars, ds_variables)\n",
    "[_, unmatch2] = cf.compare_lists(ds_variables, stream_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check deployment pressure from asset management against pressure variable in file\n",
    "# press = pf.pressure_var(ds, list(ds.coords.keys()))\n",
    "# if press is None:\n",
    "#     press = pf.pressure_var(ds, list(ds.data_vars.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # calculate mean pressure from data, excluding outliers +/- 3 SD\n",
    "# try:\n",
    "#     pressure = ds[press]\n",
    "#     num_dims = len(pressure.dims)\n",
    "#     if len(pressure) > 1:\n",
    "#         # reject NaNs\n",
    "#         p_nonan = pressure.values[~np.isnan(pressure.values)]\n",
    "\n",
    "#         # reject fill values\n",
    "#         p_nonan_nofv = p_nonan[p_nonan != pressure._FillValue]\n",
    "\n",
    "#         # reject data outside of global ranges\n",
    "#         try:\n",
    "#             [pg_min, pg_max] = cf.get_global_ranges(rd, press)\n",
    "#             if pg_min is not None and pg_max is not None:\n",
    "#                     pgr_ind = cf.reject_global_ranges(p_nonan_nofv, pg_min, pg_max)\n",
    "#                     p_nonan_nofv_gr = p_nonan_nofv[pgr_ind]\n",
    "#             else:\n",
    "#                 p_nonan_nofv_gr = p_nonan_nofv\n",
    "#         except Exception: \n",
    "#                 print('uFrame is not responding to request for global ranges. Try again later.')\n",
    "#                 p_nonan_nofv_gr = p_nonan_nofv\n",
    "\n",
    "#         if (len(p_nonan_nofv_gr) > 0) and (num_dims == 1):\n",
    "#             [press_outliers, pressure_mean, _, pressure_max, _, _] = cf.variable_statistics(p_nonan_nofv_gr, 3)\n",
    "#             pressure_mean = round(pressure_mean, 2)\n",
    "#             pressure_max = round(pressure_max, 2)\n",
    "#         elif (len(p_nonan_nofv_gr) > 0) and (num_dims > 1):\n",
    "#             print('variable has more than 1 dimension')\n",
    "#             press_outliers = 'not calculated: variable has more than 1 dimension'\n",
    "#             pressure_mean = round(np.nanmean(p_nonan_nofv_gr), 2)\n",
    "#             pressure_max = round(np.nanmax(p_nonan_nofv_gr), 2)\n",
    "#         else:\n",
    "#             press_outliers = None\n",
    "#             pressure_mean = None\n",
    "#             pressure_max = None\n",
    "#             if len(pressure) > 0 and len(p_nonan) == 0:\n",
    "#                 notes.append('Pressure variable all NaNs')\n",
    "#             elif len(pressure) > 0 and len(p_nonan) > 0 and len(p_nonan_nofv) == 0:\n",
    "#                 notes.append('Pressure variable all fill values')\n",
    "#             elif len(pressure) > 0 and len(p_nonan) > 0 and len(p_nonan_nofv) > 0 and len(p_nonan_nofv_gr) == 0:\n",
    "#                 notes.append('Pressure variable outside of global ranges')\n",
    "\n",
    "#     else:  # if there is only 1 data point\n",
    "#         press_outliers = 0\n",
    "#         pressure_mean = round(ds[press].values.tolist()[0], 2)\n",
    "#         pressure_max = round(ds[press].values.tolist()[0], 2)\n",
    "\n",
    "#     try:\n",
    "#         pressure_units = pressure.units\n",
    "#     except AttributeError:\n",
    "#         pressure_units = 'no units attribute for pressure'\n",
    "\n",
    "#     if pressure_mean:\n",
    "#         node = refdes.split('-')[1]\n",
    "#         if ('WFP' in node) or ('MOAS' in subsite):\n",
    "#             pressure_compare = int(round(pressure_max))\n",
    "#         else:\n",
    "#             pressure_compare = int(round(pressure_mean))\n",
    "\n",
    "#         if pressure_units == '0.001 dbar':\n",
    "#             pressure_max = round((pressure_max / 1000), 2)\n",
    "#             pressure_mean = round((pressure_mean / 1000), 2)\n",
    "#             notes.append('Pressure converted from 0.001 dbar to dbar for pressure comparison')\n",
    "#     else:\n",
    "#         pressure_compare = None\n",
    "\n",
    "#     if (not deploy_depth) or (not pressure_mean):\n",
    "#         pressure_diff = None\n",
    "#     else:\n",
    "#         pressure_diff = pressure_compare - deploy_depth\n",
    "\n",
    "# except KeyError:\n",
    "#     press = 'no seawater pressure in file'\n",
    "#     pressure_diff = None\n",
    "#     pressure_mean = None\n",
    "#     pressure_max = None\n",
    "#     pressure_compare = None\n",
    "#     press_outliers = None\n",
    "#     pressure_units = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add files and info to dictionary\n",
    "filenames = data['deployments'][deployment]['method'][method]['stream'][data_stream][\n",
    "    'file'].keys()\n",
    "if fname not in filenames:\n",
    "    data['deployments'][deployment]['method'][method]['stream'][data_stream]['file'][\n",
    "        fname] = OrderedDict(\n",
    "        file_downloaded=pd.to_datetime(elements[0]).strftime('%Y-%m-%dT%H:%M:%S'),\n",
    "        file_coordinates=list(ds.coords.keys()),\n",
    "        sampling_rate_seconds=sampling_rt_sec,\n",
    "        sampling_rate_details=rates,\n",
    "        data_start=data_start,\n",
    "        data_stop=data_stop,\n",
    "        time_gaps=gap_list,\n",
    "        unique_timestamps=time_test,\n",
    "        n_timestamps=len_time,\n",
    "        n_days=n_days,\n",
    "        notes=notes,\n",
    "        ascending_timestamps=time_ascending,\n",
    "        pressure_comparison=dict(pressure_mean=pressure_mean, units=pressure_units,\n",
    "                                 num_outliers=press_outliers, diff=pressure_diff,\n",
    "                                 pressure_max=pressure_max, variable=press,\n",
    "                                 pressure_compare=pressure_compare),\n",
    "        vars_in_file=ds_variables,\n",
    "        vars_not_in_file=[x for x in unmatch1 if 'time' not in x],\n",
    "        vars_not_in_db=unmatch2,\n",
    "        sci_var_stats=OrderedDict())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
